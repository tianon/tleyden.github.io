<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: coreos | Seven Story Rabbit Hole]]></title>
  <link href="http://tleyden.github.io/blog/categories/coreos/atom.xml" rel="self"/>
  <link href="http://tleyden.github.io/"/>
  <updated>2014-11-13T19:17:39-08:00</updated>
  <id>http://tleyden.github.io/</id>
  <author>
    <name><![CDATA[Traun Leyden]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CoreOS with Nvidia CUDA GPU drivers]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/"/>
    <updated>2014-11-04T07:08:00-08:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers</id>
    <content type="html"><![CDATA[<p>This will walk you through installing the Nvidia GPU kernel module and CUDA drivers on a docker container running inside of CoreOS.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/coreos-nvidia-gpu.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS on an AWS GPU instance</h2>

<ul>
<li><p>Launch a new EC2 instance</p></li>
<li><p>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-7c8b3f14</strong> (CoreOS-stable-410.1.0-hvm)</p></li>
<li><p>Select the GPU instances: <strong>g2.2xlarge</strong></p></li>
<li><p>Increase root EBS store from 8 GB &ndash;> 20 GB to give yourself some breathing room</p></li>
</ul>


<h2>ssh into CoreOS instance</h2>

<p>Find the public ip of the EC2 instance launched above, and ssh into it:</p>

<p><code>
$ ssh -A core@ec2-54-80-24-46.compute-1.amazonaws.com
</code></p>

<h2>Run Ubuntu 12 docker container in privileged mode</h2>

<p><code>
$ sudo docker run --privileged=true -i -t ubuntu:12.04 /bin/bash
</code></p>

<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<p><code>
$ apt-get update
$ apt-get install build-essential wget git
</code></p>

<h2>Prepare CoreOS kernel source</h2>

<p><strong>Clone CoreOS kernel repository</strong></p>

<p><code>
$ mkdir -p /usr/src/kernels
$ cd /usr/src/kernels
$ git clone https://github.com/coreos/linux.git
</code></p>

<p><strong>Get CoreOS kernel version</strong></p>

<p><code>
$ uname -a
Linux ip-10-183-54-167.ec2.internal 3.15.8+ #2 SMP Fri Sep 26 08:37:17 UTC 2014 x86_64 Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux
</code></p>

<p>The CoreOS kernel version is <strong>3.15.8</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<p><code>
$ cd linux
$ git checkout remotes/origin/coreos/v3.15.8
</code></p>

<p><strong>Create kernel configuration file</strong></p>

<p><code>
$ zcat /proc/config.gz &gt; /usr/src/kernels/linux/.config
</code></p>

<p><strong>Prepare kernel source for building modules</strong></p>

<p><code>
$ cd /usr/src/kernels/linux
$ make modules_prepare
</code></p>

<p>Now you should be ready to install the nvidia driver.</p>

<h2>Install nvidia driver</h2>

<p><strong>Download</strong></p>

<p><code>
$ mkdir -p /opt/nvidia
$ cd /opt/nvidia
$ wget http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run
</code></p>

<p><strong>Unpack</strong></p>

<p><code>
$ chmod +x cuda_6.5.14_linux_64.run
$ mkdir nvidia_installers
$ ./cuda_6.5.14_linux_64.run -extract=`pwd`/nvidia_installers
</code></p>

<p><strong>Install</strong></p>

<p><code>
$ cd nvidia_installers
$ ./NVIDIA-Linux-x86_64-340.29.run --kernel-source-path=/usr/src/kernels/linux/
</code></p>

<p><strong>Installer Questions</strong></p>

<ul>
<li>Install NVidia&rsquo;s 32-bit compatibility libraries? <strong>YES</strong></li>
<li>Would you like to run nvidia-xconfig? <strong>NO</strong></li>
</ul>


<p>If everything worked, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/nvidia_driver_installed.png" alt="nvidia drivers installed" /></p>

<h2>Load nvidia kernel module</h2>

<p><code>
$ modprobe nvidia
</code></p>

<p>No errors should be returned.  Verify it&rsquo;s loaded by running:</p>

<p><code>
$ lsmod | grep -i nvidia
</code></p>

<p>and you should see:</p>

<p><code>
nvidia              10533711  0
i2c_core               41189  2 nvidia,i2c_piix4
</code></p>

<h2>Install CUDA</h2>

<p><code>
$ ./cuda-linux64-rel-6.5.14-18749181.run
$ ./cuda-samples-linux-6.5.14-18745345.run
</code></p>

<h2>Verify CUDA</h2>

<p><code>
$ cd /usr/local/cuda/samples/1_Utilities/deviceQuery
$ make
$ ./deviceQuery   
</code></p>

<p>You should see the following output:</p>

<p><code>
deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
Result = PASS
</code></p>

<p>Congratulations!  You now have a docker container running under CoreOS that can access the GPU.</p>

<h1>Appendix A: Using Core OS Alpha</h1>

<p>The instructions above were for an older version of CoreOS.  The following instructions are for Core OS Alpha, and might possibly work on the current version of CoreOS stable (444.5.0).  Only the parts that differ from above steps are listed:</p>

<h2>Launch CoreOS Alpha on an AWS GPU instance</h2>

<ul>
<li>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-66e6680e</strong> (CoreOS-alpha-490.0.0-hvm)</li>
</ul>


<h2>Run Ubuntu 14 docker container in privileged mode</h2>

<p><code>
$ sudo docker run --privileged=true -i -t ubuntu:14.04 /bin/bash
</code></p>

<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<p>In order to match the version of gcc that was used to build the CoreOS kernel.  (gcc 4.7)</p>

<p><code>
$ apt-get update
$ apt-get install gcc-4.7 g++-4.7 wget git make dpkg-dev
</code></p>

<p><strong>Set gcc 4.7 as default</strong></p>

<p><code>
$ update-alternatives --remove gcc /usr/bin/gcc-4.8
$ update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.7 60 --slave /usr/bin/g++ g++ /usr/bin/g++-4.7
$ update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 40 --slave /usr/bin/g++ g++ /usr/bin/g++-4.8
</code></p>

<p><strong>Verify</strong></p>

<p><code>
$ update-alternatives --config gcc
</code></p>

<p>It should list gcc 4.7 with an asterisk next to it:</p>

<p><code>
* 0            /usr/bin/gcc-4.7   60        auto mode
</code></p>

<h2>Prepare CoreOS kernel source</h2>

<p><strong>Get CoreOS kernel version</strong></p>

<p><code>
$ uname -a
Linux ip-10-11-167-200.ec2.internal 3.17.2+ #2 SMP Tue Nov 4 04:15:48 UTC 2014 x86_64 Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux
</code></p>

<p>The CoreOS kernel version is <strong>3.17.2</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<p><code>
$ cd linux
$ git checkout remotes/origin/coreos/v3.17.2
</code></p>

<h1>Appendix B: Expose GPU to other docker containers</h1>

<p>If you need <em>other</em> docker containers on this CoreOS instance to be able to access the GPU, you can do the following steps.</p>

<p><em>Note:</em> you need to be using CoreOS-alpha-490.0.0 or later, since this requires Docker 1.3 to work.</p>

<p><strong>Exit docker container</strong></p>

<p><code>
$ exit
</code></p>

<p>You should be back to your CoreOS shell.</p>

<p><strong>Add nvidia device nodes</strong></p>

<p><code>
$ wget https://gist.githubusercontent.com/tleyden/74f593a0beea300de08c/raw/95ed93c5751a989e58153db6f88c35515b7af120/nvidia_devices.sh
$ chmod +x nvida_devices.sh
$ sudo ./nvida_devices.sh
</code></p>

<p><strong>Verify device nodes</strong></p>

<p><code>
$ ls -alh /dev | grep -i nvidia
crw-rw-rw-  1 root root  251,   0 Nov  5 16:37 nvidia-uvm
crw-rw-rw-  1 root root  195,   0 Nov  5 16:37 nvidia0
crw-rw-rw-  1 root root  195, 255 Nov  5 16:37 nvidiactl
</code></p>

<p><strong>Launch docker containers</strong></p>

<p>When you launch other docker containers on the same CoreOS instance, to allow them to access the GPU device you will need to add the following arguments:</p>

<p><code>
$ sudo docker run -ti --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm tleyden5iwx/ubuntu-cuda /bin/bash
</code></p>

<p>A complete example is available in <a href="http://tleyden.github.io/blog/2014/10/25/docker-on-aws-gpu-ubuntu-14-dot-04-slash-cuda-6-dot-5/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a>.  You can pick up at th <strong>Run GPU enabled docker image</strong> step.</p>

<h2>References</h2>

<ul>
<li><a href="https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4">https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4</a> &ndash; Thanks Сергей!</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Couchbase Cluster Under CoreOS on AWS]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/"/>
    <updated>2014-11-01T12:16:00-07:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws</id>
    <content type="html"><![CDATA[<p>Here are instructions on how to fire up a Couchbase Server 2.2 cluster running under CoreOS on AWS CloudFormation.  You will end up with the following system:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase-coreos-onion.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS instances via AWS Cloud Formation</h2>

<p>Click the &ldquo;Launch Stack&rdquo; button to launch your CoreOS instances via AWS Cloud Formation:</p>

<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/couchbase-coreos/coreos-stable-pv.template"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png"></a></p>

<p><em>NOTE: this is hardcoded to use the us-east-1 region, so if you need a different region, you should edit the URL accordingly</em></p>

<p>Use the following parameters in the form:</p>

<ul>
<li><strong>ClusterSize</strong>: 3 nodes (default)</li>
<li><strong>Discovery URL</strong>:  as it says, you need to grab a new token from <a href="https://discovery.etcd.io/new">https://discovery.etcd.io/new</a> and paste it in the box.</li>
<li><strong>KeyPair</strong>:  use whatever you normally use to start EC2 instances.  For this discussion, let&rsquo;s assumed you used <code>aws</code>, which corresponds to a file you have on your laptop called <code>aws.cer</code></li>
</ul>


<h2>ssh into a CoreOS instance</h2>

<p>Go to the AWS console under EC2 instances and find the public ip of one of your newly launched CoreOS instances.</p>

<p>Choose any one of them (it doesn&rsquo;t matter which), and ssh into it as the <strong>core</strong> user with the cert provided in the previous step:</p>

<p><code>
$ ssh -i aws.cer -A core@ec2-54-83-80-161.compute-1.amazonaws.com
</code></p>

<h2>Sanity check</h2>

<p>Let&rsquo;s make sure the CoreOS cluster is healthy first:</p>

<p><code>
$ fleetctl list-machines
</code></p>

<p>This should return a list of machines in the cluster, like this:</p>

<p><code>
MACHINE         IP              METADATA
03b08680...     10.33.185.16    -
209a8a2e...     10.164.175.9    -
25dd84b7...     10.13.180.194   -
</code></p>

<h2>Download cluster-init script</h2>

<p><code>
$ wget https://raw.githubusercontent.com/tleyden/couchbase-server-coreos/master/2.2/scripts/cluster-init.sh
$ chmod +x cluster-init.sh
</code></p>

<h2>Launch cluster</h2>

<p><code>
$ ./cluster-init.sh -n 3 -u "user:passw0rd"
</code></p>

<p>Where:</p>

<ul>
<li><strong>-n</strong> the total number of couchbase nodes to start &mdash; should correspond to number of ec2 instances (eg, 3)</li>
<li><strong>-u</strong> the username and password as a single string, delimited by a colon (:)</li>
</ul>


<p>Replace <code>user:passw0rd</code> with a sensible username and password.  It <strong>must</strong> be colon separated, with no spaces.  The password itself must be at least 6 characters.</p>

<p>Once this command completes, your cluster will be in the process of launching.</p>

<h2>Verify</h2>

<p>To check the status of your cluster, run:</p>

<p><code>
$ fleetctl list-units
</code></p>

<p>You should see four units, all as active.</p>

<p><code>
UNIT                        MACHINE             ACTIVE  SUB
couchbase_bootstrap_node.service                375d98b9.../10.63.168.35    active  running
couchbase_bootstrap_node_announce.service       375d98b9.../10.63.168.35    active  running
couchbase_node.1.service                        8cf54d4d.../10.187.61.136   active  running
couchbase_node.2.service                        b8cf0ed6.../10.179.161.76   active  running
</code></p>

<h2>Rebalance Couchbase Cluster</h2>

<p><strong>Login to Couchbase Server Web Admin</strong></p>

<ul>
<li>Find the public ip of any of your CoreOS instances via the AWS console</li>
<li>In a browser, go to <code>http://&lt;instance_public_ip&gt;:8091</code></li>
<li>Login with the username/password you provided above</li>
</ul>


<p>After logging in, your Server Nodes tab should look like this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_admin_ui_prerebalance.png" alt="screenshot" /></p>

<p><strong>Kick off initial rebalance</strong></p>

<ul>
<li>Click server nodes</li>
<li>Click &ldquo;Rebalance&rdquo;</li>
</ul>


<p>After the rebalance is complete, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_admin_ui_post_rebalance.png" alt="screenshot" /></p>

<h1>References</h1>

<ul>
<li><a href="https://gist.github.com/dustin/6605182">How I built couchbase 2.2 for docker</a> by <a href="https://twitter.com/dlsspy">@dlsspy</a></li>
<li><a href="https://github.com/tleyden/couchbase-server-coreos">https://github.com/tleyden/couchbase-server-coreos</a></li>
<li><a href="https://registry.hub.docker.com/u/ncolomer/couchbase/">https://registry.hub.docker.com/u/ncolomer/couchbase/</a></li>
<li><a href="https://github.com/lifegadget/docker-couchbase">https://github.com/lifegadget/docker-couchbase</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
