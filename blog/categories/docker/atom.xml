<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Seven Story Rabbit Hole]]></title>
  <link href="http://tleyden.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://tleyden.github.io/"/>
  <updated>2015-04-25T05:09:54+00:00</updated>
  <id>http://tleyden.github.io/</id>
  <author>
    <name><![CDATA[Traun Leyden]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting up Octopress under Docker]]></title>
    <link href="http://tleyden.github.io/blog/2015/04/25/setting-up-octopress-under-docker/"/>
    <updated>2015-04-25T03:57:00+00:00</updated>
    <id>http://tleyden.github.io/blog/2015/04/25/setting-up-octopress-under-docker</id>
    <content type="html"><![CDATA[<p>I got a new computer last week.  It&rsquo;s the latest macbook retina, and I needed to refresh because I wanted a bigger SSD drive (and after having an SSD drive, I&rsquo;ll never go back)</p>

<p>Anyway, I&rsquo;m trying to get my Octopress blog going again, and oh my God, what a nightmare.  Octopress was working beautifully for me for years, and then all of the sudden I am at the edge of Ruby Dependency Hell staring at an Octopress giving me eight fingers.</p>

<p>With the help of Docker, I&rsquo;ve managed to tame this eight legged beast, barely.</p>

<h2>Run Docker</h2>

<p>See <a href="https://docs.docker.com/installation/">Installing Docker</a> for instructions.</p>

<p>This blog post assumes you <strong>already have an Octopress git repo</strong>.  If you are starting from scratch, then check out <a href="http://tleyden.github.io/blog/2013/09/07/octopress-setup-part-i/">Octopress Setup Part I</a> to become even more confused.</p>

<h2>Install Octopress Docker image</h2>

<p><code>
$ docker run -ti tleyden5iwx/octopress /bin/bash
</code></p>

<p>After this point, the rest of the instructions assume that you are executing commands from inside the Docker Container.</p>

<h2>Delete Octopress dir + clone your Octopress repo</h2>

<p>The Docker container will contain an Octopress directory, but it&rsquo;s not needed.</p>

<p>From within the container:</p>

<p><code>
$ cd /root
$ rm -rf octopress/
$ git clone https://github.com/your-github-username/your-github-username.github.io.git octopress
$ cd octopress/
</code></p>

<p>Now, switch to the source branch (which contains the content)</p>

<p><code>
$ git checkout source
</code></p>

<p>Re-install dependencies:</p>

<p><code>
$ bundle install
</code></p>

<p>Prevent ASCII encoding errors:</p>

<p><code>
$ export LC_ALL=C.UTF-8
</code></p>

<p><strong>Clone deploy directory</strong></p>

<p><code>
$ git clone https://github.com/your-github-username/your-github-username.github.io.git _deploy
</code></p>

<h2>Rake preview</h2>

<p>As a smoke test, run:</p>

<p><code>
$ bundle exec rake preview
</code></p>

<p>NOTE: I have no idea why <code>bundle exec</code> is required here, I just used this in reponse to a previous error message and it&rsquo;s accompanying suggestion.</p>

<p>If this gives no errors, that&rsquo;s a good sign.</p>

<h2>Create a new blog post</h2>

<p><code>
$ bundle exec rake new_post["Setting up Octopress under Docker"]
</code></p>

<p>It will tell you the path to the blog post.  Now open the file in your favorite editor and add contect.</p>

<h2>Push to Source branch</h2>

<p>The source branch has the <strong>source markdown content</strong>.  It&rsquo;s actually the most important thing to preserve, because the HTML can always be regnerated from it.</p>

<p><code>
$ git push origin source
</code></p>

<h2>Deploy to Master branch</h2>

<p>The master branch contains the <strong>rendered HTML content</strong>.  Here&rsquo;s how to push it up to your github pages repo (remember, in an earlier step you cloned your github pages repo at <a href="https://github.com/your-github-username/your-github-username.github.io.git">https://github.com/your-github-username/your-github-username.github.io.git</a>)</p>

<p><code>
$ bundle exec rake generate &amp;&amp; bundle exec rake deploy
</code></p>

<p>After the above command, the changes should be visible on your github pages blog (eg, your-username.github.io)</p>

<h2>Common errors</h2>

<p>If you get:</p>

<p><code>
YAML Exception reading 2014-04-09-a-successful-git-branching-model-with-enterprise-support.markdown: invalid byte sequence in US-ASCII
</code></p>

<p>Run:</p>

<p><code>
$ export LC_ALL=C.UTF-8
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx proxy for Sync Gateway using Confd]]></title>
    <link href="http://tleyden.github.io/blog/2015/03/21/nginx-proxy-for-sync-gateway-using-confd/"/>
    <updated>2015-03-21T15:25:00+00:00</updated>
    <id>http://tleyden.github.io/blog/2015/03/21/nginx-proxy-for-sync-gateway-using-confd</id>
    <content type="html"><![CDATA[<p>This will walk you through setting up Sync Gateway behind nginx.  The nginx conf will be auto generated based on Sync Gateway status.</p>

<h3>Launch CoreOS instances on EC2</h3>

<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/couchbase-coreos/sync_gateway.template"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png"></a></p>

<p>Recommended values:</p>

<ul>
<li><strong>ClusterSize</strong>: 3 nodes (default)</li>
<li><strong>Discovery URL</strong>:  as it says, you need to grab a new token from <a href="https://discovery.etcd.io/new">https://discovery.etcd.io/new</a> and paste it in the box.</li>
<li><strong>KeyPair</strong>: the name of the AWS keypair you want to use.  If you haven&rsquo;t already, you&rsquo;ll want to upload your local ssh key into AWS and create a named keypair.</li>
</ul>


<h3>Wait until instances are up</h3>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cloud-formation-create-complete.png" alt="screenshot" /></p>

<h3>ssh into a CoreOS instance</h3>

<p>Go to the AWS console under EC2 instances and find the public ip of one of your newly launched CoreOS instances.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/ec2-instances-coreos.png" alt="screenshot" /></p>

<p>Choose any one of them (it doesn&rsquo;t matter which), and ssh into it as the <strong>core</strong> user with the cert provided in the previous step:</p>

<p><code>
$ ssh -i aws.cer -A core@ec2-54-83-80-161.compute-1.amazonaws.com
</code></p>

<h2>Spin up Sync Gateway containers</h2>

<p><code>
$ etcdctl set /couchbase.com/enable-code-refresh true
$ sudo docker run --net=host tleyden5iwx/couchbase-cluster-go update-wrapper sync-gw-cluster launch-sgw --num-nodes=2 --config-url=http://git.io/hFwa --in-memory-db
</code></p>

<h2>Verify etcd entries</h2>

<p><code>
$ etcdctl ls --recursive /
...
/couchbase.com/sync-gw-node-state
/couchbase.com/sync-gw-node-state/10.169.70.114
/couchbase.com/sync-gw-node-state/10.231.220.110
</code></p>

<h2>Create data volume container</h2>

<p><code>
$ wget https://raw.githubusercontent.com/lordelph/confd-demo/master/confdata.service
$ fleetctl start confdata.service
</code></p>

<h2>Launch sync-gateway-nginx-confd.service</h2>

<p><code>
$ wget https://raw.githubusercontent.com/lordelph/confd-demo/master/confd.service
$ sed -i -e 's/lordelph\/confd-demo/tleyden5iwx\/sync-gateway-nginx-confd/' confd.service
$ fleetctl start confd.service
</code></p>

<h2>Launch nginx service</h2>

<p><code>
$ wget https://raw.githubusercontent.com/lordelph/confd-demo/master/nginx.service
$ fleetctl start nginx.service
</code></p>

<h2>Verify</h2>

<p>Try a basic http get.</p>

<p><code>
$ nginx_ip=`fleetctl list-units | grep -i nginx | awk '{print $2}' | awk -F/ '{print $2}'`
$ curl $nginx_ip
{"couchdb":"Welcome","vendor":{"name":"Couchbase Sync Gateway","version":1},"version":"Couchbase Sync Gateway/master(a47a17f)"}
</code></p>

<p>Add &lsquo;-v&rsquo; flag to see which Sync Gateway server is servicing the request</p>

<p><code>
$ curl -v $nginx_ip
...
X-Handler: 10.231.220.110:4984
...
</code></p>

<p>If you repeat that a few more times, you should see different ip addresses for the handler.</p>

<p><strong>Take a sync gateway out of rotation</strong></p>

<p><code>
$ fleetctl stop sync_gw_node@1.service sync_gw_sidekick@1.service
</code></p>

<p>Now try hitting nginx again, and should not see the Sync Gw that you just shutdown as a handler.</p>

<p><code>
$ curl -v $nginx_ip
...
X-Handler: 10.231.220.114:4984
...
</code></p>

<p><strong>Put sync gateway back into rotation</strong></p>

<p><code>
$ fleetctl start sync_gw_node@1.service sync_gw_sidekick@1.service
</code></p>

<p>Now try hitting nginx again, and should again see the Sync Gw that you just restarted as being a handler.</p>

<p><code>
$ curl -v $nginx_ip
...
X-Handler: 10.231.220.110:4984
...
</code></p>

<h2>References</h2>

<ul>
<li><a href="http://blog.dixo.net/2015/02/load-balancing-with-coreos">http://blog.dixo.net/2015/02/load-balancing-with-coreos</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running a CBFS cluster on CoreOS]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/14/running-cbfs/"/>
    <updated>2014-11-14T06:43:00+00:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/14/running-cbfs</id>
    <content type="html"><![CDATA[<p>This will walk you through getting a cbfs cluster up and running.</p>

<h2>What is CBFS?</h2>

<p>cbfs is a distributed filesystem on top of Couchbase Server, not unlike Mongo&rsquo;s GridFS or Riak&rsquo;s CS.</p>

<p>Here&rsquo;s a typical deployment architecture:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cbfs-overview.png" alt="cbfs overview" /></p>

<p>Although not shown, all cbfs daemons can communicate with all Couchbase Server instances.</p>

<p>It is not required to run cbfs on the same machine as Couchbase Server, but it <em>is</em> meant to be run in the same data center as Couchbase Server.</p>

<p>If you want a deeper understanding of how cbfs works, check the <a href="http://labs.couchbase.com/cbfs/">cbfs presentation</a> or this <a href="http://dustin.sallings.org/2012/09/27/cbfs.html">blog post</a>.</p>

<h2>Kick off a Couchbase Cluster</h2>

<p>cbfs depends on having a Couchbase cluster running.</p>

<p>Follow all of the steps in <a href="http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/">Running Couchbase Cluster Under CoreOS on AWS</a> to kick off a 3 node Couchbase cluster.</p>

<h2>Add security groups</h2>

<p>A few ports will need to be opened up for cbfs.</p>

<p>Go to the AWS console and edit the Couchbase-CoreOS-CoreOSSecurityGroup-xxxx security group and add the following rules:</p>

<p>```
Type             Protocol  Port Range Source</p>

<hr />

<p>Custom TCP Rule  TCP       8484       Custom IP: sg-6e5a0d04 (copy and paste from port 4001 rule)
Custom TCP Rule  TCP       8423       Custom IP: sg-6e5a0d04
```</p>

<p>At this point your security group should look like this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/security_group_cbfs.png" alt="security group" /></p>

<h1>Create a new bucket for cbfs</h1>

<p><strong>Open Couchbase Server Admin UI</strong></p>

<p>In the AWS EC2 console, find the public IP of one of the instances (it doesn&rsquo;t matter which)</p>

<p>In your browser, go to <code>http://&lt;public_ip&gt;:8091/</code></p>

<p><strong>Create Bucket</strong></p>

<p>Go to Data Buckets / Create New Bucket</p>

<p>Enter <strong>cbfs</strong> for the name of the bucket.</p>

<p>Leave all other settings as default.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cbfs_create_bucket.png" alt="create bucket" /></p>

<h2>ssh in</h2>

<p>In the AWS EC2 console, find the public IP of one of the instances (it doesn&rsquo;t matter which)</p>

<p>ssh into one of the machines:</p>

<p><code>
$ ssh -A core@&lt;public_ip&gt;
</code></p>

<h2>Run cbfs</h2>

<p><strong>Create a volume dir</strong></p>

<p>Since the fileystem of a docker container is not meant for high throughput io, a volume should be used for cbfs.</p>

<p>Create a directory on the host OS (i.e., on the Core OS instance)</p>

<p><code>
$ sudo mkdir -p /var/lib/cbfs/data
$ sudo chown -R core:core /var/lib/cbfs
</code></p>

<p>This will be mounted by the docker container in the next step.</p>

<p><strong>Generate fleet unit files</strong></p>

<p><code>
$ wget https://gist.githubusercontent.com/tleyden/d70161c3827cb8b788a8/raw/8f6c81f0095b0007565e9b205e90afb132552060/cbfs_node.service.template
$ for i in `seq 1 3`; do cp cbfs_node.service.template cbfs_node.$i.service; done
</code></p>

<p><strong>Start cbfs on all cluster nodes</strong></p>

<p><code>
$ fleetctl start cbfs_node.*.service
</code></p>

<p>Run <code>fleetctl list-units</code> to list the  units running in your cluster.  You should have the following:</p>

<p><code>
$ fleetctl list-units
UNIT                                            MACHINE                         ACTIVE  SUB
cbfs_node.1.service                             6ecff20c.../10.51.177.81        active  running
cbfs_node.2.service                             b8eb6653.../10.79.155.153       active  running
cbfs_node.3.service                             02d48afd.../10.186.172.24       active  running
couchbase_bootstrap_node.service                02d48afd.../10.186.172.24       active  running
couchbase_bootstrap_node_announce.service       02d48afd.../10.186.172.24       active  running
couchbase_node.1.service                        6ecff20c.../10.51.177.81        active  running
couchbase_node.2.service                        b8eb6653.../10.79.155.153       active  running
</code></p>

<p><strong>View cbfs output</strong></p>

<p><code>
$ fleetctl journal cbfs_node.1.service
2014/11/14 23:18:58 Connecting to couchbase bucket cbfs at http://10.51.177.81:8091/
2014/11/14 23:18:58 Error checking view version: MCResponse status=KEY_ENOENT, opcode=GET, opaque=0, msg: Not found
2014/11/14 23:18:58 Installing new version of views (old version=0)
2014/11/14 23:18:58 Listening to web requests on :8484 as server 10.51.177.81
2014/11/14 23:18:58 Error removing 10.51.177.81's task list: MCResponse status=KEY_ENOENT, opcode=DELETE, opaque=0, msg: Not found
2014/11/14 23:19:05 Error updating space used: Expected 1 result, got []
</code></p>

<h2>Run cbfs client</h2>

<p>Run a bash shell in a docker container that has <code>cbfsclient</code> pre-installed:</p>

<p><code>
$ sudo docker run -ti --net=host tleyden5iwx/cbfs /bin/bash
</code></p>

<p><strong>Upload a file</strong></p>

<p>From within the docker container launched in the previous step:</p>

<p>```</p>

<h1>echo &ldquo;foo&rdquo; > foo</h1>

<h1>ip=$(hostname -i | tr -d &lsquo; &rsquo;)</h1>

<h1>cbfsclient <a href="http://$ip:8484/">http://$ip:8484/</a> upload foo /foo</h1>

<p>```</p>

<p>There should be no errors.  If you run <code>fleetctl journal cbfs_node.1.service</code> again on the CoreOS instance, you should see log messages like:</p>

<p><code>
2014/11/14 21:51:43 Recorded myself as an owner of e242ed3bffccdf271b7fbaf34ed72d089537b42f: result=success
</code></p>

<p><strong>List directory</strong></p>

<p>```</p>

<h1>cbfsclient <a href="http://$ip:8484/">http://$ip:8484/</a> ls /</h1>

<p>foo
```</p>

<p>It should list the foo file we uploaded earlier.</p>

<p>Congratulations!  You now have cbfs up and running.</p>

<h2>References</h2>

<ul>
<li><a href="http://dustin.sallings.org/2012/09/27/cbfs.html">cbfs &ndash; a couchbase large object store</a></li>
<li><a href="http://labs.couchbase.com/cbfs/">cbfs presentation</a></li>
<li><a href="http://cbfs-ext.hq.couchbase.com/dist/">cbfs binary downloads</a></li>
<li><a href="http://github.com/couchbaselabs/cbfs">cbfs github repo</a></li>
<li><a href="https://github.com/couchbaselabs/cbfs/issues/132">cbfs question</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CoreOS with Nvidia CUDA GPU drivers]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/"/>
    <updated>2014-11-04T07:08:00+00:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers</id>
    <content type="html"><![CDATA[<p>This will walk you through installing the Nvidia GPU kernel module and CUDA drivers on a docker container running inside of CoreOS.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/coreos-nvidia-gpu.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS on an AWS GPU instance</h2>

<ul>
<li><p>Launch a new EC2 instance</p></li>
<li><p>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-f669f29e</strong> (CoreOS stable 494.4.0 (HVM))</p></li>
<li><p>Select the GPU instances: <strong>g2.2xlarge</strong></p></li>
<li><p>Increase root EBS store from 8 GB &ndash;> 20 GB to give yourself some breathing room</p></li>
</ul>


<h2>ssh into CoreOS instance</h2>

<p>Find the public ip of the EC2 instance launched above, and ssh into it:</p>

<p><code>
$ ssh -A core@ec2-54-80-24-46.compute-1.amazonaws.com
</code></p>

<h2>Run Ubuntu 14 docker container in privileged mode</h2>

<p><code>
$ sudo docker run --privileged=true -i -t ubuntu:14.04 /bin/bash
</code></p>

<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<p>In order to match the version of gcc that was used to build the CoreOS kernel.  (gcc 4.7)</p>

<p>```</p>

<h1>apt-get update</h1>

<h1>apt-get install gcc-4.7 g++-4.7 wget git make dpkg-dev</h1>

<p>```</p>

<p><strong>Set gcc 4.7 as default</strong></p>

<p>```</p>

<h1>update-alternatives &mdash;remove gcc /usr/bin/gcc-4.8</h1>

<h1>update-alternatives &mdash;install /usr/bin/gcc gcc /usr/bin/gcc-4.7 60 &mdash;slave /usr/bin/g++ g++ /usr/bin/g++-4.7</h1>

<h1>update-alternatives &mdash;install /usr/bin/gcc gcc /usr/bin/gcc-4.8 40 &mdash;slave /usr/bin/g++ g++ /usr/bin/g++-4.8</h1>

<p>```</p>

<p><strong>Verify</strong></p>

<p>```</p>

<h1>update-alternatives &mdash;config gcc</h1>

<p>```</p>

<p>It should list gcc 4.7 with an asterisk next to it:</p>

<p><code>
* 0            /usr/bin/gcc-4.7   60        auto mode
</code></p>

<h2>Prepare CoreOS kernel source</h2>

<p><strong>Clone CoreOS kernel repository</strong></p>

<p><code>
$ mkdir -p /usr/src/kernels
$ cd /usr/src/kernels
$ git clone https://github.com/coreos/linux.git
</code></p>

<p><strong>Find CoreOS kernel version</strong></p>

<p>```</p>

<h1>uname -a</h1>

<p>Linux ip-10-11-167-200.ec2.internal 3.17.2+ #2 SMP Tue Nov 4 04:15:48 UTC 2014 x86_64 Intel&reg; Xeon&reg; CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux
```</p>

<p>The CoreOS kernel version is <strong>3.17.2</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<p>```</p>

<h1>cd linux</h1>

<h1>git checkout remotes/origin/coreos/v3.17.2</h1>

<p>```</p>

<p><strong>Create kernel configuration file</strong></p>

<p>```</p>

<h1>zcat /proc/config.gz > /usr/src/kernels/linux/.config</h1>

<p>```</p>

<p><strong>Prepare kernel source for building modules</strong></p>

<p>```</p>

<h1>make modules_prepare</h1>

<p>```</p>

<p>Now you should be ready to install the nvidia driver.</p>

<p><strong>Hack the kernel version</strong></p>

<p>In order to avoid <a href="https://gist.github.com/tleyden/2a46a86056e476976a8e#file-gistfile1-txt-L956">nvidia: version magic errors</a>, the following hack is required:</p>

<p>```</p>

<h1>sed -i -e &rsquo;s/3.17.2/3.17.2+/&lsquo; include/generated/utsrelease.h</h1>

<p>```</p>

<p>I&rsquo;ve <a href="https://groups.google.com/d/msg/coreos-user/CSp_wSywmI4/CBHwocj8v9oJ">posted to the CoreOS Group</a> to ask why this hack is needed.</p>

<h2>Install nvidia driver</h2>

<p><strong>Download</strong></p>

<p>```</p>

<h1>mkdir -p /opt/nvidia</h1>

<h1>cd /opt/nvidia</h1>

<h1>wget <a href="http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run">http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run</a></h1>

<p>```</p>

<p><strong>Unpack</strong></p>

<p>```</p>

<h1>chmod +x cuda_6.5.14_linux_64.run</h1>

<h1>mkdir nvidia_installers</h1>

<h1>./cuda_6.5.14_linux_64.run -extract=<code>pwd</code>/nvidia_installers</h1>

<p>```</p>

<p><strong>Install</strong></p>

<p>```</p>

<h1>cd nvidia_installers</h1>

<h1>./NVIDIA-Linux-x86_64-340.29.run &mdash;kernel-source-path=/usr/src/kernels/linux/</h1>

<p>```</p>

<p><strong>Installer Questions</strong></p>

<ul>
<li>Install NVidia&rsquo;s 32-bit compatibility libraries? <strong>YES</strong></li>
<li>Would you like to run nvidia-xconfig? <strong>NO</strong></li>
</ul>


<p>If everything worked, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/nvidia_driver_installed.png" alt="nvidia drivers installed" /></p>

<p>your <code>/var/log/nvidia-installer.log</code> should look something like <a href="https://gist.github.com/tleyden/6d585fb8ab08154949c8">this</a></p>

<h2>Load nvidia kernel module</h2>

<p>```</p>

<h1>modprobe nvidia</h1>

<p>```</p>

<p>No errors should be returned.  Verify it&rsquo;s loaded by running:</p>

<p>```</p>

<h1>lsmod | grep -i nvidia</h1>

<p>```</p>

<p>and you should see:</p>

<p><code>
nvidia              10533711  0
i2c_core               41189  2 nvidia,i2c_piix4
</code></p>

<h2>Install CUDA</h2>

<p>In order to fully verify that the kernel module is working correctly, install the CUDA drivers + library and run a device query.</p>

<p>To install CUDA:</p>

<p>```</p>

<h1>./cuda-linux64-rel-6.5.14-18749181.run</h1>

<h1>./cuda-samples-linux-6.5.14-18745345.run</h1>

<p>```</p>

<h2>Verify CUDA</h2>

<p>```</p>

<h1>cd /usr/local/cuda/samples/1_Utilities/deviceQuery</h1>

<h1>make</h1>

<h1>./deviceQuery</h1>

<p>```</p>

<p>You should see the following output:</p>

<p><code>
deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
Result = PASS
</code></p>

<p>Congratulations!  You now have a docker container running under CoreOS that can access the GPU.</p>

<h1>Appendix: Expose GPU to other docker containers</h1>

<p>If you need <em>other</em> docker containers on this CoreOS instance to be able to access the GPU, you can do the following steps.</p>

<p><strong>Exit docker container</strong></p>

<p>```</p>

<h1>exit</h1>

<p>```</p>

<p>You should be back to your CoreOS shell.</p>

<p><strong>Add nvidia device nodes</strong></p>

<p><code>
$ wget https://gist.githubusercontent.com/tleyden/74f593a0beea300de08c/raw/95ed93c5751a989e58153db6f88c35515b7af120/nvidia_devices.sh
$ chmod +x nvidia_devices.sh
$ sudo ./nvidia_devices.sh
</code></p>

<p><strong>Verify device nodes</strong></p>

<p><code>
$ ls -alh /dev | grep -i nvidia
crw-rw-rw-  1 root root  251,   0 Nov  5 16:37 nvidia-uvm
crw-rw-rw-  1 root root  195,   0 Nov  5 16:37 nvidia0
crw-rw-rw-  1 root root  195, 255 Nov  5 16:37 nvidiactl
</code></p>

<p><strong>Launch docker containers</strong></p>

<p>When you launch other docker containers on the same CoreOS instance, to allow them to access the GPU device you will need to add the following arguments:</p>

<p><code>
$ sudo docker run -ti --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm tleyden5iwx/ubuntu-cuda /bin/bash
</code></p>

<h2>References</h2>

<ul>
<li><a href="https://github.com/tleyden/tleyden.github.io/blob/6d71759cc5e530efcae10d9c6012dd217f76795c/source/_posts/2014-11-04-coreos-with-nvidia-cuda-gpu-drivers.markdown">Previous version of this blog post</a></li>
<li><a href="https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4">CoreOS Google Group thread</a> &ndash; Thanks Сергей!</li>
<li><a href="http://tleyden.github.io/blog/2014/10/25/docker-on-aws-gpu-ubuntu-14-dot-04-slash-cuda-6-dot-5/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Couchbase Cluster Under CoreOS on AWS]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/"/>
    <updated>2014-11-01T12:16:00+00:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws</id>
    <content type="html"><![CDATA[<p>Here are instructions on how to fire up a Couchbase Server cluster running under CoreOS on AWS CloudFormation.  You will end up with the following system:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase-coreos-onion.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS instances via AWS Cloud Formation</h2>

<p>Click the &ldquo;Launch Stack&rdquo; button to launch your CoreOS instances via AWS Cloud Formation:</p>

<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/couchbase-coreos/couchbase_server.template"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png"></a></p>

<p><em>NOTE: this is hardcoded to use the us-east-1 region, so if you need a different region, you should edit the URL accordingly</em></p>

<p>Use the following parameters in the form:</p>

<ul>
<li><strong>ClusterSize</strong>: 3 nodes (default)</li>
<li><strong>Discovery URL</strong>:  as it says, you need to grab a new token from <a href="https://discovery.etcd.io/new">https://discovery.etcd.io/new</a> and paste it in the box.</li>
<li><strong>KeyPair</strong>:  use whatever you normally use to start EC2 instances.  For this discussion, let&rsquo;s assumed you used <code>aws</code>, which corresponds to a file you have on your laptop called <code>aws.cer</code></li>
</ul>


<h2>ssh into a CoreOS instance</h2>

<p>Go to the AWS console under EC2 instances and find the public ip of one of your newly launched CoreOS instances.</p>

<p>Choose any one of them (it doesn&rsquo;t matter which), and ssh into it as the <strong>core</strong> user with the cert provided in the previous step:</p>

<p><code>
$ ssh -i aws.cer -A core@ec2-54-83-80-161.compute-1.amazonaws.com
</code></p>

<h2>Sanity check</h2>

<p>Let&rsquo;s make sure the CoreOS cluster is healthy first:</p>

<p><code>
$ fleetctl list-machines
</code></p>

<p>This should return a list of machines in the cluster, like this:</p>

<p><code>
MACHINE         IP              METADATA
03b08680...     10.33.185.16    -
209a8a2e...     10.164.175.9    -
25dd84b7...     10.13.180.194   -
</code></p>

<h2>Launch cluster</h2>

<p><code>
$ sudo docker run --net=host tleyden5iwx/couchbase-cluster-go:0.8 couchbase-fleet launch-cbs --version 3.0.1 --num-nodes 3 --userpass "user:passw0rd" --docker-tag 0.8
</code></p>

<p>Where:</p>

<ul>
<li>&mdash;version=&lt;cb-version> Couchbase Server version (3.0.1 or 2.2)</li>
<li>&mdash;num-nodes=&lt;num_nodes> number of couchbase nodes to start</li>
<li>&mdash;userpass &lt;user:pass> the username and password as a single string, delimited by a colon (:)</li>
<li>&mdash;etcd-servers=&lt;server-list>  Comma separated list of etcd servers, or omit to connect to etcd running on localhost</li>
<li>&mdash;docker-tag=&lt;docker-tag>  if present, use this docker tag for spawned containers, otherwise, default to &ldquo;latest&rdquo;</li>
</ul>


<p>Replace <code>user:passw0rd</code> with a sensible username and password.  It <strong>must</strong> be colon separated, with no spaces.  The password itself must be at least 6 characters.</p>

<p>Once this command completes, your cluster will be up and running.  The output should look something like <a href="https://gist.github.com/tleyden/bc0975778216281b80e7">this gist</a>.</p>

<h2>Verify</h2>

<p>To check the status of your cluster, run:</p>

<p><code>
$ fleetctl list-units
</code></p>

<p>You should see three units, all as active.</p>

<p><code>
UNIT                            MACHINE                         ACTIVE  SUB
couchbase_node@1.service        3c819355.../10.239.170.243      active  running
couchbase_node@2.service        782b35d4.../10.168.87.23        active  running
couchbase_node@3.service        7cd5f94c.../10.234.188.145      active  running
</code></p>

<h1>Login to Admin Web UI</h1>

<ul>
<li>Find the public ip of any of your CoreOS instances via the AWS console</li>
<li>In a browser, go to <code>http://&lt;instance_public_ip&gt;:8091</code></li>
<li>Login with the username/password you provided above</li>
</ul>


<p>You should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_admin_ui_post_rebalance.png" alt="screenshot" /></p>

<p>When CoreOS reboots a machine automatically for a security upgrade, the data will be automatically rebalanced before and after the reboot.  <a href="https://github.com/couchbaselabs/couchbase-server-docker/issues/3#issuecomment-75984021">Details + screenshots here</a>.</p>

<p>Congratulations!  You now have a 3 node Couchbase Server cluster running under CoreOS / Docker.</p>

<h1>References</h1>

<ul>
<li><a href="https://github.com/couchbaselabs/couchbase-server-docker">Dockerfiles + Fleet init scripts</a></li>
<li><a href="https://gist.github.com/dustin/6605182">How I built couchbase 2.2 for docker</a> by <a href="https://twitter.com/dlsspy">@dlsspy</a></li>
<li><a href="https://registry.hub.docker.com/u/ncolomer/couchbase/">https://registry.hub.docker.com/u/ncolomer/couchbase/</a></li>
<li><a href="https://github.com/lifegadget/docker-couchbase">https://github.com/lifegadget/docker-couchbase</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
