<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Seven Story Rabbit Hole]]></title>
  <link href="http://tleyden.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://tleyden.github.io/"/>
  <updated>2014-11-13T19:27:14-08:00</updated>
  <id>http://tleyden.github.io/</id>
  <author>
    <name><![CDATA[Traun Leyden]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CoreOS with Nvidia CUDA GPU drivers]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/"/>
    <updated>2014-11-04T07:08:00-08:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers</id>
    <content type="html"><![CDATA[<p>This will walk you through installing the Nvidia GPU kernel module and CUDA drivers on a docker container running inside of CoreOS.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/coreos-nvidia-gpu.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS on an AWS GPU instance</h2>

<ul>
<li><p>Launch a new EC2 instance</p></li>
<li><p>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-7c8b3f14</strong> (CoreOS-stable-410.1.0-hvm)</p></li>
<li><p>Select the GPU instances: <strong>g2.2xlarge</strong></p></li>
<li><p>Increase root EBS store from 8 GB &ndash;> 20 GB to give yourself some breathing room</p></li>
</ul>


<h2>ssh into CoreOS instance</h2>

<p>Find the public ip of the EC2 instance launched above, and ssh into it:</p>

<p><code>
$ ssh -A core@ec2-54-80-24-46.compute-1.amazonaws.com
</code></p>

<h2>Run Ubuntu 12 docker container in privileged mode</h2>

<p><code>
$ sudo docker run --privileged=true -i -t ubuntu:12.04 /bin/bash
</code></p>

<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<p><code>
$ apt-get update
$ apt-get install build-essential wget git
</code></p>

<h2>Prepare CoreOS kernel source</h2>

<p><strong>Clone CoreOS kernel repository</strong></p>

<p><code>
$ mkdir -p /usr/src/kernels
$ cd /usr/src/kernels
$ git clone https://github.com/coreos/linux.git
</code></p>

<p><strong>Get CoreOS kernel version</strong></p>

<p><code>
$ uname -a
Linux ip-10-183-54-167.ec2.internal 3.15.8+ #2 SMP Fri Sep 26 08:37:17 UTC 2014 x86_64 Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux
</code></p>

<p>The CoreOS kernel version is <strong>3.15.8</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<p><code>
$ cd linux
$ git checkout remotes/origin/coreos/v3.15.8
</code></p>

<p><strong>Create kernel configuration file</strong></p>

<p><code>
$ zcat /proc/config.gz &gt; /usr/src/kernels/linux/.config
</code></p>

<p><strong>Prepare kernel source for building modules</strong></p>

<p><code>
$ cd /usr/src/kernels/linux
$ make modules_prepare
</code></p>

<p>Now you should be ready to install the nvidia driver.</p>

<h2>Install nvidia driver</h2>

<p><strong>Download</strong></p>

<p><code>
$ mkdir -p /opt/nvidia
$ cd /opt/nvidia
$ wget http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run
</code></p>

<p><strong>Unpack</strong></p>

<p><code>
$ chmod +x cuda_6.5.14_linux_64.run
$ mkdir nvidia_installers
$ ./cuda_6.5.14_linux_64.run -extract=`pwd`/nvidia_installers
</code></p>

<p><strong>Install</strong></p>

<p><code>
$ cd nvidia_installers
$ ./NVIDIA-Linux-x86_64-340.29.run --kernel-source-path=/usr/src/kernels/linux/
</code></p>

<p><strong>Installer Questions</strong></p>

<ul>
<li>Install NVidia&rsquo;s 32-bit compatibility libraries? <strong>YES</strong></li>
<li>Would you like to run nvidia-xconfig? <strong>NO</strong></li>
</ul>


<p>If everything worked, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/nvidia_driver_installed.png" alt="nvidia drivers installed" /></p>

<h2>Load nvidia kernel module</h2>

<p><code>
$ modprobe nvidia
</code></p>

<p>No errors should be returned.  Verify it&rsquo;s loaded by running:</p>

<p><code>
$ lsmod | grep -i nvidia
</code></p>

<p>and you should see:</p>

<p><code>
nvidia              10533711  0
i2c_core               41189  2 nvidia,i2c_piix4
</code></p>

<h2>Install CUDA</h2>

<p><code>
$ ./cuda-linux64-rel-6.5.14-18749181.run
$ ./cuda-samples-linux-6.5.14-18745345.run
</code></p>

<h2>Verify CUDA</h2>

<p><code>
$ cd /usr/local/cuda/samples/1_Utilities/deviceQuery
$ make
$ ./deviceQuery   
</code></p>

<p>You should see the following output:</p>

<p><code>
deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
Result = PASS
</code></p>

<p>Congratulations!  You now have a docker container running under CoreOS that can access the GPU.</p>

<h1>Appendix A: Using Core OS Alpha</h1>

<p>The instructions above were for an older version of CoreOS.  The following instructions are for Core OS Alpha, and might possibly work on the current version of CoreOS stable (444.5.0).  Only the parts that differ from above steps are listed:</p>

<h2>Launch CoreOS Alpha on an AWS GPU instance</h2>

<ul>
<li>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-66e6680e</strong> (CoreOS-alpha-490.0.0-hvm)</li>
</ul>


<h2>Run Ubuntu 14 docker container in privileged mode</h2>

<p><code>
$ sudo docker run --privileged=true -i -t ubuntu:14.04 /bin/bash
</code></p>

<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<p>In order to match the version of gcc that was used to build the CoreOS kernel.  (gcc 4.7)</p>

<p><code>
$ apt-get update
$ apt-get install gcc-4.7 g++-4.7 wget git make dpkg-dev
</code></p>

<p><strong>Set gcc 4.7 as default</strong></p>

<p><code>
$ update-alternatives --remove gcc /usr/bin/gcc-4.8
$ update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.7 60 --slave /usr/bin/g++ g++ /usr/bin/g++-4.7
$ update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 40 --slave /usr/bin/g++ g++ /usr/bin/g++-4.8
</code></p>

<p><strong>Verify</strong></p>

<p><code>
$ update-alternatives --config gcc
</code></p>

<p>It should list gcc 4.7 with an asterisk next to it:</p>

<p><code>
* 0            /usr/bin/gcc-4.7   60        auto mode
</code></p>

<h2>Prepare CoreOS kernel source</h2>

<p><strong>Get CoreOS kernel version</strong></p>

<p><code>
$ uname -a
Linux ip-10-11-167-200.ec2.internal 3.17.2+ #2 SMP Tue Nov 4 04:15:48 UTC 2014 x86_64 Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux
</code></p>

<p>The CoreOS kernel version is <strong>3.17.2</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<p><code>
$ cd linux
$ git checkout remotes/origin/coreos/v3.17.2
</code></p>

<h1>Appendix B: Expose GPU to other docker containers</h1>

<p>If you need <em>other</em> docker containers on this CoreOS instance to be able to access the GPU, you can do the following steps.</p>

<p><em>Note:</em> you need to be using CoreOS-alpha-490.0.0 or later, since this requires Docker 1.3 to work.</p>

<p><strong>Exit docker container</strong></p>

<p><code>
$ exit
</code></p>

<p>You should be back to your CoreOS shell.</p>

<p><strong>Add nvidia device nodes</strong></p>

<p><code>
$ wget https://gist.githubusercontent.com/tleyden/74f593a0beea300de08c/raw/95ed93c5751a989e58153db6f88c35515b7af120/nvidia_devices.sh
$ chmod +x nvida_devices.sh
$ sudo ./nvida_devices.sh
</code></p>

<p><strong>Verify device nodes</strong></p>

<p><code>
$ ls -alh /dev | grep -i nvidia
crw-rw-rw-  1 root root  251,   0 Nov  5 16:37 nvidia-uvm
crw-rw-rw-  1 root root  195,   0 Nov  5 16:37 nvidia0
crw-rw-rw-  1 root root  195, 255 Nov  5 16:37 nvidiactl
</code></p>

<p><strong>Launch docker containers</strong></p>

<p>When you launch other docker containers on the same CoreOS instance, to allow them to access the GPU device you will need to add the following arguments:</p>

<p><code>
$ sudo docker run -ti --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm tleyden5iwx/ubuntu-cuda /bin/bash
</code></p>

<p>A complete example is available in <a href="http://tleyden.github.io/blog/2014/10/25/docker-on-aws-gpu-ubuntu-14-dot-04-slash-cuda-6-dot-5/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a>.  You can pick up at th <strong>Run GPU enabled docker image</strong> step.</p>

<h2>References</h2>

<ul>
<li><a href="https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4">https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4</a> &ndash; Thanks Сергей!</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Couchbase Cluster Under CoreOS on AWS]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/"/>
    <updated>2014-11-01T12:16:00-07:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws</id>
    <content type="html"><![CDATA[<p>Here are instructions on how to fire up a Couchbase Server 2.2 cluster running under CoreOS on AWS CloudFormation.  You will end up with the following system:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase-coreos-onion.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS instances via AWS Cloud Formation</h2>

<p>Click the &ldquo;Launch Stack&rdquo; button to launch your CoreOS instances via AWS Cloud Formation:</p>

<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/couchbase-coreos/coreos-stable-pv.template"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png"></a></p>

<p><em>NOTE: this is hardcoded to use the us-east-1 region, so if you need a different region, you should edit the URL accordingly</em></p>

<p>Use the following parameters in the form:</p>

<ul>
<li><strong>ClusterSize</strong>: 3 nodes (default)</li>
<li><strong>Discovery URL</strong>:  as it says, you need to grab a new token from <a href="https://discovery.etcd.io/new">https://discovery.etcd.io/new</a> and paste it in the box.</li>
<li><strong>KeyPair</strong>:  use whatever you normally use to start EC2 instances.  For this discussion, let&rsquo;s assumed you used <code>aws</code>, which corresponds to a file you have on your laptop called <code>aws.cer</code></li>
</ul>


<h2>ssh into a CoreOS instance</h2>

<p>Go to the AWS console under EC2 instances and find the public ip of one of your newly launched CoreOS instances.</p>

<p>Choose any one of them (it doesn&rsquo;t matter which), and ssh into it as the <strong>core</strong> user with the cert provided in the previous step:</p>

<p><code>
$ ssh -i aws.cer -A core@ec2-54-83-80-161.compute-1.amazonaws.com
</code></p>

<h2>Sanity check</h2>

<p>Let&rsquo;s make sure the CoreOS cluster is healthy first:</p>

<p><code>
$ fleetctl list-machines
</code></p>

<p>This should return a list of machines in the cluster, like this:</p>

<p><code>
MACHINE         IP              METADATA
03b08680...     10.33.185.16    -
209a8a2e...     10.164.175.9    -
25dd84b7...     10.13.180.194   -
</code></p>

<h2>Download cluster-init script</h2>

<p><code>
$ wget https://raw.githubusercontent.com/tleyden/couchbase-server-coreos/master/2.2/scripts/cluster-init.sh
$ chmod +x cluster-init.sh
</code></p>

<h2>Launch cluster</h2>

<p><code>
$ ./cluster-init.sh -n 3 -u "user:passw0rd"
</code></p>

<p>Where:</p>

<ul>
<li><strong>-n</strong> the total number of couchbase nodes to start &mdash; should correspond to number of ec2 instances (eg, 3)</li>
<li><strong>-u</strong> the username and password as a single string, delimited by a colon (:)</li>
</ul>


<p>Replace <code>user:passw0rd</code> with a sensible username and password.  It <strong>must</strong> be colon separated, with no spaces.  The password itself must be at least 6 characters.</p>

<p>Once this command completes, your cluster will be in the process of launching.</p>

<h2>Verify</h2>

<p>To check the status of your cluster, run:</p>

<p><code>
$ fleetctl list-units
</code></p>

<p>You should see four units, all as active.</p>

<p><code>
UNIT                        MACHINE             ACTIVE  SUB
couchbase_bootstrap_node.service                375d98b9.../10.63.168.35    active  running
couchbase_bootstrap_node_announce.service       375d98b9.../10.63.168.35    active  running
couchbase_node.1.service                        8cf54d4d.../10.187.61.136   active  running
couchbase_node.2.service                        b8cf0ed6.../10.179.161.76   active  running
</code></p>

<h2>Rebalance Couchbase Cluster</h2>

<p><strong>Login to Couchbase Server Web Admin</strong></p>

<ul>
<li>Find the public ip of any of your CoreOS instances via the AWS console</li>
<li>In a browser, go to <code>http://&lt;instance_public_ip&gt;:8091</code></li>
<li>Login with the username/password you provided above</li>
</ul>


<p>After logging in, your Server Nodes tab should look like this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_admin_ui_prerebalance.png" alt="screenshot" /></p>

<p><strong>Kick off initial rebalance</strong></p>

<ul>
<li>Click server nodes</li>
<li>Click &ldquo;Rebalance&rdquo;</li>
</ul>


<p>After the rebalance is complete, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_admin_ui_post_rebalance.png" alt="screenshot" /></p>

<p>Congratulations!  You now have an n-node Couchbase Server cluster running under CoreOS / Docker.</p>

<p>If you&rsquo;re feeling bold, go ahead and <a href="https://www.youtube.com/watch?v=spxdtmdoUwM">pound on it</a>.</p>

<h1>References</h1>

<ul>
<li><a href="https://gist.github.com/dustin/6605182">How I built couchbase 2.2 for docker</a> by <a href="https://twitter.com/dlsspy">@dlsspy</a></li>
<li><a href="https://github.com/tleyden/couchbase-server-coreos">https://github.com/tleyden/couchbase-server-coreos</a></li>
<li><a href="https://registry.hub.docker.com/u/ncolomer/couchbase/">https://registry.hub.docker.com/u/ncolomer/couchbase/</a></li>
<li><a href="https://github.com/lifegadget/docker-couchbase">https://github.com/lifegadget/docker-couchbase</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Caffe on AWS GPU instance via Docker]]></title>
    <link href="http://tleyden.github.io/blog/2014/10/25/running-caffe-on-aws-gpu-instance-via-docker/"/>
    <updated>2014-10-25T20:42:00-07:00</updated>
    <id>http://tleyden.github.io/blog/2014/10/25/running-caffe-on-aws-gpu-instance-via-docker</id>
    <content type="html"><![CDATA[<p>This is a tutorial to help you get the <a href="http://caffe.berkeleyvision.org/">Caffe deep learning framework</a> up and running on a GPU-powered AWS instance running inside a Docker container.</p>

<h2>Architecture</h2>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/caffe_docker_aws_onion.png" alt="architecture diagram" /></p>

<h2>Setup host</h2>

<p>Before you can start your docker container, you will need to go <strong>deeper down the rabbit hole</strong>.</p>

<p>You&rsquo;ll first need to complete the steps here:</p>

<p><a href="http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">Setting up an Ubuntu 14.04 box running on a GPU-enabled AWS instance</a></p>

<p>After you&rsquo;re done, you&rsquo;ll end up with a host OS with the following properties:</p>

<ul>
<li>A GPU enabled AWS instance running Ubuntu 14.04</li>
<li>Nvidia kernel module</li>
<li>Nvidia device drivers</li>
<li>CUDA 6.5 installed and verified</li>
</ul>


<h2>Install Docker</h2>

<p>Once your host OS is setup, you&rsquo;re ready to install docker.  (version 1.3 at the time of this writing)</p>

<p>Setup the key for the docker repo:</p>

<p><code>
$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9
</code></p>

<p>Add the docker repo:</p>

<p><code>
$ sudo sh -c "echo deb https://get.docker.com/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list"
$ sudo apt-get update
</code></p>

<p>Install docker:</p>

<p><code>
$ sudo apt-get install lxc-docker
</code></p>

<h2>Run the docker container</h2>

<p><strong>Find your nvidia devices</strong></p>

<p><code>
$ ls -la /dev | grep nvidia
</code></p>

<p>You should see:</p>

<p><code>
crw-rw-rw-  1 root root    195,   0 Oct 25 19:37 nvidia0
crw-rw-rw-  1 root root    195, 255 Oct 25 19:37 nvidiactl
crw-rw-rw-  1 root root    251,   0 Oct 25 19:37 nvidia-uvm
</code></p>

<p>You&rsquo;ll have to adapt the <code>DOCKER_NVIDIA_DEVICES</code> variable below to match your particular devices.</p>

<p>Here&rsquo;s how to start the docker container:</p>

<p><code>
$ DOCKER_NVIDIA_DEVICES="--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm"
$ sudo docker run -ti $DOCKER_NVIDIA_DEVICES tleyden5iwx/caffe-gpu /bin/bash
</code></p>

<p>It&rsquo;s a large docker image, so this might take a few minutes, depending on your network connection.</p>

<h2>Run caffe test suite</h2>

<p>After the above <code>docker run</code> command completes, your shell will now be inside a docker container that has Caffe installed.</p>

<p>You&rsquo;ll want run the Caffe test suite and make sure it passes.  This will validate your environment, including your GPU drivers.</p>

<p><code>
$ cd /opt/caffe
$ make test &amp;&amp; make runtest
</code></p>

<p><strong>Expected Result:</strong> <code>... [  PASSED  ] 838 tests.</code></p>

<h2>Run the MNIST LeNet example</h2>

<p>A more comprehensive way to verify your environment is to train the MNIST LeNet example:</p>

<p><code>
$ cd /opt/caffe/data/mnist
$ ./get_mnist.sh
$ cd /opt/caffe
$ ./examples/mnist/create_mnist.sh
$ ./examples/mnist/train_lenet.sh
</code></p>

<p>This will take a few minutes.</p>

<p><strong>Expected output:</strong></p>

<p><code>
libdc1394 error: Failed to initialize libdc1394
I1018 17:02:23.552733    66 caffe.cpp:90] Starting Optimization
I1018 17:02:23.553583    66 solver.cpp:32] Initializing solver from parameters:
... lots of output ...
I1018 17:17:58.684598    66 caffe.cpp:102] Optimization Done.
</code></p>

<p>Congratulations, you&rsquo;ve got GPU-powered Caffe running in a docker container &mdash; celebrate with a cup of <a href="http://www.yelp.com/biz/philz-coffee-berkeley-2">Philz</a>!</p>

<h1>References</h1>

<ul>
<li><a href="https://registry.hub.docker.com/u/tleyden5iwx/caffe-gpu">tleyden5iwx/caffe-gpu</a> Caffe Docker image (GPU)</li>
<li><a href="https://registry.hub.docker.com/u/tleyden5iwx/caffe">tleyden5iwx/caffe</a> Caffe Docker image (CPU-only)</li>
<li><a href="http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a></li>
<li><a href="http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">CUDA 6.5 on AWS GPU Instance Running Ubuntu 14.04</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running couchbase cluster under docker]]></title>
    <link href="http://tleyden.github.io/blog/2013/11/14/running-couchbase-cluster-under-docker/"/>
    <updated>2013-11-14T15:45:00-08:00</updated>
    <id>http://tleyden.github.io/blog/2013/11/14/running-couchbase-cluster-under-docker</id>
    <content type="html"><![CDATA[<p>This tutorial will show you how to run a cluster of Couchbase Servers, where each node is running inside of a docker image.</p>

<p><img src="http://cl.ly/image/2G0h381N3o42/docker%20couchbase%20cluster.png" alt="Diagram" /></p>

<p>This probably looks like <em>a lot</em> of layers, and you might be wondering if this will make your system crawl &mdash; but bear in mind that the Docker virtualization model is very lightweight, and so basically everything under CoreOS has very little resource overhead.</p>

<h2>Install Docker and dependencies</h2>

<p>If you are on OSX and don&rsquo;t have Docker installed, check out <a href="http://tleyden.github.io/blog/2013/11/12/docker-on-osx/">Install Docker on OSX</a> before proceeding.</p>

<h2>Edit vagrant file to add port mappings</h2>

<p>In order to access all the Couchbase Server nodes from the host, <em>which doesn&rsquo;t currently work</em>, you would need to add the following entries to your Vagrantfile:</p>

<p><code>
config.vm.network "forwarded_port", guest: 8091, host: 8091
config.vm.network "forwarded_port", guest: 8092, host: 8092
config.vm.network "forwarded_port", guest: 11210, host: 11210
</code></p>

<p>As mentioned, accessing all of the Couchbase Server nodes from the host does not currently work.  However, I think at least some of these entries are needed, so to be on the safe side just add all of them.</p>

<h2>Start CoreOS and ssh in</h2>

<p>Execute the following commands in the directory where you have your CoreOS Vagrantfile.  In my case, I have it under <code>~/Tools/coreos-vagrant</code> and it contains a Vagrantfile, a README.md file, and a few others.</p>

<p>Start CoreOS</p>

<p><code>
$ vagrant up
</code></p>

<p>SSH into CoreOS</p>

<p><code>
$ vagrant ssh
</code></p>

<h2>Start docker image for a single node</h2>

<p>Here&rsquo;s how to fire up the first docker image</p>

<p><code>
docker run -i -t -d -p 11210:11210 -p 8091:7081 -p 8092:8092 dustin/couchbase:latest
</code>
and you should see:</p>

<p><code>
Unable to find image 'dustin/couchbase:latest' (tag: latest) locally
Pulling repository dustin/couchbase
9e0279ab340d: Download complete
...
845987ce946b
</code></p>

<p>Find the name of the docker instance by running <code>$ docker ps</code></p>

<p><code>
core@localhost ~ $ docker ps
CONTAINER ID        IMAGE                     COMMAND                CREATED              STATUS              PORTS                                                                      NAMES
845987ce946b        dustin/couchbase:latest   /bin/sh -c /usr/loca   About a minute ago   Up About a minute   0.0.0.0:11210-&gt;11210/tcp, 0.0.0.0:8091-&gt;7081/tcp, 0.0.0.0:8092-&gt;8092/tcp   purple_kangaroo
</code></p>

<p>In this case it&rsquo;s <em>purple_kangaroo</em>.</p>

<p>Now take a look at the logs for that docker instance:</p>

<p><code>
core@localhost ~ $ docker logs purple_kangaroo
/opt/couchbase/etc/couchbase_init.d: 45: ulimit: error setting limit (Operation not permitted)
/opt/couchbase/etc/couchbase_init.d: 47: ulimit: error setting limit (Operation not permitted)
 * Started couchbase-server
Starting cluster
SUCCESS: init 127.0.0.1
SUCCESS: bucket-create
</code></p>

<p>If you only want to run one Couchbase Server node, you are pretty much done and you can skip to the section below to login to the Couchbase Server admin</p>

<p>If you want to run a cluster of Couchbase Server nodes, read on.</p>

<h2>Start docker images for other nodes</h2>

<p><code>
$ docker run -i -t -d -link purple_kangaroo:alpha dustin/couchbase:latest
</code></p>

<p>This will start another node that is linked to the initial node, and will be in the same cluster.  There is some Weird Magic behind the scenes that makes that all work.</p>

<h1>Go to Couchbase Server admin</h1>

<p>Open <a href="http://localhost:8091/">http://localhost:8091/</a> in your browser, and you should see a login screen, where the default credentials are Administrator/password.</p>

<p>After you login, you should see the Admin UI with three nodes in your cluster:</p>

<p><img src="http://cl.ly/image/2K3i1v2w3H0E/Screen%20Shot%202013-11-14%20at%204.47.18%20PM.png" alt="Screenshot" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker on OSX]]></title>
    <link href="http://tleyden.github.io/blog/2013/11/12/docker-on-osx/"/>
    <updated>2013-11-12T21:01:00-08:00</updated>
    <id>http://tleyden.github.io/blog/2013/11/12/docker-on-osx</id>
    <content type="html"><![CDATA[<p><img src="http://cl.ly/image/302X103n330L/docker.png" alt="Docker.png" /></p>

<h1>Goal</h1>

<p>Get Docker running under OSX.</p>

<h2>Environment</h2>

<ul>
<li><p>OSX Mavericks</p></li>
<li><p>Macbook Retina</p></li>
</ul>


<h2>Understanding the tools</h2>

<ul>
<li>Vagrant is a tool to provision VMs.</li>
<li>Virtualbox is VM software.</li>
<li>CoreOS is an operating system based on linux that&rsquo;s all about running docker.</li>
</ul>


<h2>Install VirtualBox + Vagrant</h2>

<ul>
<li><p>Install VirtualBox via <a href="http://download.virtualbox.org/virtualbox/4.3.2/VirtualBox-4.3.2-90405-OSX.dmg">VirtualBox OSX installer</a></p></li>
<li><p>Install Vagrant via <a href="https://dl.bintray.com/mitchellh/vagrant/vagrant_1.6.2.dmg">Vagrant OSX installer</a></p></li>
</ul>


<h2>Vagrant Smoke Test: run Ubuntu Precise</h2>

<p>To make sure Vagrant is installed correctly, run the following commands to see if Ubuntu Precise comes up and you are able to ssh into it.</p>

<p><code>
vagrant init precise32 http://files.vagrantup.com/precise32.box
vagrant up
vagrant ssh
</code></p>

<p>If that doesn&rsquo;t work, you should probably fix it before moving on.  Otherwise, keep reading.</p>

<h2>Install/Run CoreOS</h2>

<p><code>
git clone https://github.com/coreos/coreos-vagrant/
cd coreos-vagrant
vagrant up
</code></p>

<h2>SSH into CoreOS</h2>

<p><code>
vagrant ssh
</code></p>

<h2>Run stock ubuntu docker image</h2>

<p><code>
docker run -t -i ubuntu bash
</code></p>

<p>After running this, you should be ssh&rsquo;d inside of an Ubuntu shell, running under docker (running inside of VirtualBox .. etc, all the way up the onion)</p>
]]></content>
  </entry>
  
</feed>
